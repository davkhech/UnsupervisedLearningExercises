{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(doc):\n",
    "    \"\"\"\n",
    "    Convert a document into a list of words (also remove stop words and punctuation)\n",
    "    \"\"\"\n",
    "    regex = r'\\w+'\n",
    "    words = re.findall(regex, doc)\n",
    "    words = filter(lambda word: word not in stop and len(word) > 2 and len(re.findall(r'\\d+', word)) == 0, words)\n",
    "    words = map(lambda word: lemma.lemmatize(word.lower()), words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_corpus_features(cleaned_docs, min_count=2, max_frequency=0.94):\n",
    "    \"\"\"\n",
    "    Build a dictionary from words in dictonary to their indices.\n",
    "    \"\"\"\n",
    "    docs = len(cleaned_docs)\n",
    "    ans = Counter()\n",
    "    cnt = Counter()\n",
    "    freq = Counter()\n",
    "    \n",
    "    for doc in cleaned_docs:\n",
    "        for word in set(doc):\n",
    "            freq[word] += 1\n",
    "        for word in doc:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "    for word, c in cnt.iteritems():\n",
    "        if freq[word] > min_count and freq[word] / docs <= max_frequency:\n",
    "            ans[word] = cnt[word]\n",
    "    return dict(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(cleaned_doc, index_to_word):\n",
    "    features = np.zeros(len(index_to_word), dtype=np.int)\n",
    "    cnt = Counter(cleaned_doc)\n",
    "    ind = 0\n",
    "    for w in index_to_word:\n",
    "        features[ind] = cnt[w]\n",
    "        ind += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, index_to_word, word_to_display):\n",
    "    words = np.array(index_to_word)\n",
    "    top = words[np.argsort(model.components_, axis=1)[:, -word_to_display:]]\n",
    "    print top.shape\n",
    "    for topic in top:\n",
    "        print '\\n'\n",
    "        for w in topic:\n",
    "            print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "train_data = trainset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 113 ms, total: 10.5 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cleaned_docs = [clean(doc) for doc in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 50.1 ms, total: 1.24 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_dict = build_corpus_features(cleaned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 s, sys: 602 ms, total: 34.8 s\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index_to_word = zip(*sorted(corpus_dict.items(), key=lambda x: x[1]))[0][-5000:]\n",
    "\n",
    "featurized_docs = np.array([featurize(doc, index_to_word) for doc in cleaned_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 1.23 s, total: 1min 43s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda = LatentDirichletAllocation(n_topics=20, learning_method='online', max_iter=20)\n",
    "lda.fit(featurized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 21)\n",
      "\n",
      "\n",
      "audio\n",
      "great\n",
      "all\n",
      "red\n",
      "smith\n",
      "cable\n",
      "ranger\n",
      "asking\n",
      "good\n",
      "best\n",
      "edu\n",
      "for\n",
      "excellent\n",
      "box\n",
      "condition\n",
      "shipping\n",
      "sell\n",
      "offer\n",
      "price\n",
      "sale\n",
      "new\n",
      "\n",
      "\n",
      "year\n",
      "surface\n",
      "contest\n",
      "lunar\n",
      "flight\n",
      "cost\n",
      "data\n",
      "rocket\n",
      "station\n",
      "moon\n",
      "shuttle\n",
      "orbit\n",
      "system\n",
      "mission\n",
      "earth\n",
      "launch\n",
      "satellite\n",
      "air\n",
      "nasa\n",
      "the\n",
      "space\n",
      "\n",
      "\n",
      "but\n",
      "home\n",
      "think\n",
      "back\n",
      "well\n",
      "went\n",
      "say\n",
      "day\n",
      "know\n",
      "time\n",
      "going\n",
      "would\n",
      "last\n",
      "people\n",
      "they\n",
      "one\n",
      "and\n",
      "team\n",
      "said\n",
      "the\n",
      "year\n",
      "\n",
      "\n",
      "posting\n",
      "contact\n",
      "site\n",
      "available\n",
      "faq\n",
      "request\n",
      "pub\n",
      "modem\n",
      "fax\n",
      "computer\n",
      "email\n",
      "anonymous\n",
      "ftp\n",
      "address\n",
      "send\n",
      "internet\n",
      "list\n",
      "mail\n",
      "com\n",
      "edu\n",
      "max\n",
      "\n",
      "\n",
      "division\n",
      "played\n",
      "playoff\n",
      "pick\n",
      "shot\n",
      "baseball\n",
      "san\n",
      "chicago\n",
      "point\n",
      "run\n",
      "year\n",
      "fan\n",
      "goal\n",
      "hockey\n",
      "league\n",
      "season\n",
      "the\n",
      "win\n",
      "play\n",
      "player\n",
      "game\n",
      "\n",
      "\n",
      "human\n",
      "but\n",
      "christ\n",
      "truth\n",
      "church\n",
      "faith\n",
      "weapon\n",
      "belief\n",
      "law\n",
      "word\n",
      "life\n",
      "bible\n",
      "religion\n",
      "believe\n",
      "people\n",
      "say\n",
      "one\n",
      "jesus\n",
      "christian\n",
      "the\n",
      "god\n",
      "\n",
      "\n",
      "support\n",
      "cpu\n",
      "char\n",
      "tape\n",
      "int\n",
      "ibm\n",
      "ide\n",
      "rom\n",
      "the\n",
      "floppy\n",
      "port\n",
      "bus\n",
      "controller\n",
      "mac\n",
      "hard\n",
      "system\n",
      "do\n",
      "scsi\n",
      "card\n",
      "disk\n",
      "drive\n",
      "\n",
      "\n",
      "the\n",
      "use\n",
      "system\n",
      "point\n",
      "way\n",
      "know\n",
      "well\n",
      "mean\n",
      "this\n",
      "could\n",
      "may\n",
      "many\n",
      "like\n",
      "question\n",
      "what\n",
      "think\n",
      "make\n",
      "key\n",
      "people\n",
      "one\n",
      "would\n",
      "\n",
      "\n",
      "number\n",
      "general\n",
      "president\n",
      "report\n",
      "press\n",
      "administration\n",
      "center\n",
      "year\n",
      "health\n",
      "technology\n",
      "american\n",
      "research\n",
      "university\n",
      "april\n",
      "program\n",
      "information\n",
      "study\n",
      "national\n",
      "state\n",
      "new\n",
      "the\n",
      "\n",
      "\n",
      "army\n",
      "would\n",
      "german\n",
      "anti\n",
      "population\n",
      "peace\n",
      "land\n",
      "jewish\n",
      "military\n",
      "attack\n",
      "arab\n",
      "people\n",
      "country\n",
      "right\n",
      "israeli\n",
      "government\n",
      "state\n",
      "jew\n",
      "war\n",
      "israel\n",
      "the\n",
      "\n",
      "\n",
      "monitor\n",
      "could\n",
      "mail\n",
      "problem\n",
      "looking\n",
      "driver\n",
      "any\n",
      "also\n",
      "doe\n",
      "help\n",
      "use\n",
      "get\n",
      "work\n",
      "need\n",
      "like\n",
      "one\n",
      "please\n",
      "know\n",
      "would\n",
      "anyone\n",
      "thanks\n",
      "\n",
      "\n",
      "something\n",
      "well\n",
      "take\n",
      "much\n",
      "really\n",
      "say\n",
      "see\n",
      "even\n",
      "way\n",
      "want\n",
      "problem\n",
      "would\n",
      "good\n",
      "thing\n",
      "think\n",
      "know\n",
      "you\n",
      "time\n",
      "like\n",
      "get\n",
      "one\n",
      "\n",
      "\n",
      "germany\n",
      "azeri\n",
      "city\n",
      "history\n",
      "azerbaijan\n",
      "homosexual\n",
      "gay\n",
      "child\n",
      "genocide\n",
      "armenia\n",
      "men\n",
      "water\n",
      "nazi\n",
      "greek\n",
      "turkey\n",
      "the\n",
      "people\n",
      "woman\n",
      "muslim\n",
      "turkish\n",
      "armenian\n",
      "\n",
      "\n",
      "police\n",
      "ground\n",
      "privacy\n",
      "court\n",
      "encryption\n",
      "state\n",
      "criminal\n",
      "firearm\n",
      "case\n",
      "used\n",
      "this\n",
      "crime\n",
      "bike\n",
      "use\n",
      "right\n",
      "control\n",
      "government\n",
      "law\n",
      "gun\n",
      "car\n",
      "the\n",
      "\n",
      "\n",
      "fast\n",
      "performance\n",
      "unit\n",
      "board\n",
      "dod\n",
      "meg\n",
      "high\n",
      "faster\n",
      "serial\n",
      "used\n",
      "power\n",
      "pin\n",
      "number\n",
      "mode\n",
      "two\n",
      "data\n",
      "the\n",
      "memory\n",
      "speed\n",
      "chip\n",
      "bit\n",
      "\n",
      "\n",
      "welcome\n",
      "third\n",
      "final\n",
      "jim\n",
      "colorado\n",
      "andrew\n",
      "must\n",
      "year\n",
      "remark\n",
      "conference\n",
      "pittsburgh\n",
      "angeles\n",
      "vancouver\n",
      "first\n",
      "los\n",
      "date\n",
      "pt\n",
      "may\n",
      "second\n",
      "power\n",
      "period\n",
      "\n",
      "\n",
      "peter\n",
      "man\n",
      "letter\n",
      "catholic\n",
      "new\n",
      "unit\n",
      "two\n",
      "appears\n",
      "mark\n",
      "english\n",
      "master\n",
      "guide\n",
      "art\n",
      "cross\n",
      "title\n",
      "text\n",
      "article\n",
      "copy\n",
      "reference\n",
      "the\n",
      "book\n",
      "\n",
      "\n",
      "rlk\n",
      "treatment\n",
      "pit\n",
      "chi\n",
      "min\n",
      "medical\n",
      "det\n",
      "bank\n",
      "qax\n",
      "okz\n",
      "bos\n",
      "myers\n",
      "van\n",
      "doctor\n",
      "buf\n",
      "disease\n",
      "patient\n",
      "chz\n",
      "bxn\n",
      "giz\n",
      "bhj\n",
      "\n",
      "\n",
      "format\n",
      "system\n",
      "display\n",
      "source\n",
      "software\n",
      "widget\n",
      "user\n",
      "set\n",
      "using\n",
      "code\n",
      "application\n",
      "server\n",
      "entry\n",
      "available\n",
      "version\n",
      "use\n",
      "image\n",
      "the\n",
      "program\n",
      "window\n",
      "file\n",
      "\n",
      "\n",
      "laser\n",
      "processor\n",
      "word\n",
      "bar\n",
      "product\n",
      "cause\n",
      "clock\n",
      "throw\n",
      "edge\n",
      "table\n",
      "font\n",
      "msg\n",
      "deleted\n",
      "the\n",
      "print\n",
      "black\n",
      "picture\n",
      "option\n",
      "use\n",
      "printer\n",
      "food\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, index_to_word, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
